[SearchHeadLevel - Accelerated DataModels with All Time Searching Enabled]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4 
counttype = number of events
cron_schedule = 0 7 * * *
description = Chance the alert requires action? High. Having an accelerated data model running searches every 5 minutes over all time can cause serious issues
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /services/apps/local | search disabled=0 `comment("Look for accelerated data models that have all time set, this can cripple an indexer cluster if the index is large enough. \
Since the datamodel REST API only returns globally shared data models *or* data models seen in the current application scope we use a map to run the search many times")`\
| fields title \
| map maxsearches=200 search="| rest /servicesNS/admin/$title$/data/models  | search acceleration!=0 AND acceleration.earliest_time=0 | fields acceleration, disabled, eai:acl.app, eai:acl.owner, eai:acl.sharing,  title, eai:userName, acceleration.earliest_time" \
| search `comment("Since the search was run many times we now have duplicates for any globally shared datamodel, remove the duplicates")`\
| dedup title, eai:acl.app
disabled = 1

[AllSplunkEnterpriseLevel - Email Sending Failures]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = 3 * * * *
description = Chance the alert requires action? High. Ideally this action shouldn't be using email but this should fire when the email server is throwing errors
dispatch.earliest_time = -1h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Find any failures to send emails due to either the size of the email or the email server not working or similar")`\
index=_internal `splunkenterprisehosts` "stderr from " python sendemail.py sourcetype=splunkd\
| dedup message \
| fields _time, host, _raw\
| head 20
disabled = 1

[AllSplunkEnterpriseLevel - Splunk Servers throwing runScript errors]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 57 10 * * *
description = Chance the alert requires action? Low. Splunk Enterprise servers are throwing an error related to running a script, this may or may not be an issue...
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("runScript errors are an indicator of a potential issue with an application")`\
index=_internal `splunkenterprisehosts` sourcetype=splunkd source="/opt/splunk/var/log/splunk/splunkd.log" \
"ERROR ScriptRunner - stderr from '/opt/splunk/bin/python /opt/splunk/bin/runScript.py execute'" OR "ERROR ExecProcessor - message from \"python *ERROR*"\
`comment("Do not include INFO level messages from standard error/out")`\
NOT " INFO " \
| cluster showcount=true \
| fields host, _raw, cluster_count
disabled = 1

[AllSplunkEnterpriseLevel - Splunk Servers with time skew]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = 0 20 * * 5
description = Chance the alert requires action? Moderate. Some level of time skew is occurring on the Splunk Enterprise servers
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Unusually large time skews may indicate an issue, often the endpoint server is the problem here rather than a Splunk issue...")` \
index=_internal `splunkenterprisehosts` sourcetype=splunkd source="/opt/splunk/var/log/splunk/splunkd.log" "A time skew of approximately "\
| rex "Peer:https?://(?P<hostname>[^:]+)" \
| top hostname \
| where count>10
disabled = 1

[AllSplunkEnterpriseLevel - Splunkd Crash Logs Have Appeared in Production]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = 47 3 * * 1
description = Chance the alert requires action? High. Production crashes are usually a problem
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","title","severity"]
display.general.type = statistics
display.page.search.mode = fast
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("crash logs on the Splunk enterprise servers are usually an issue, this may require a support ticket")`\
index=_internal `splunkenterprisehosts` sourcetype=splunkd_crash_log\
| top source, host, sourcetype
disabled = 1

[AllSplunkEnterpriseLevel - ulimit on Splunk enterprise servers is below 8192]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = 17 3 * * *
description = Chance the alert requires action? High. ulimit should be 8192 or above as per the http://docs.splunk.com/Documentation/Splunk/latest/Installation/Systemrequirements
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","indextime","host","loglevel","search_id"]
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Any Splunk enterprise servers running less than 8192 file descriptors can result in a crash, therefore we watch the ulimit numbers on startup")` \
index=_internal "ulimit" "open files:" `splunkenterprisehosts`  sourcetype=splunkd \
| rex "(?P<nooffiles>\d+) files" \
| where nooffiles<8192
disabled = 1

[AllSplunkLevel - Application Installation Failures From Deployment Manager]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = 0 9 * * *
description = Chance the alert requires action? Moderate. Applications have failed to install from the deployment server and this may require investigation
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Deployment clients can pull applications down but they may not install the application so we watch for this error to see if an application failed to install")` \
index=_internal sourcetype=splunkd action=Install\
| dedup app, ip | search result!="ok" \
| top limit=100 ip app result \
| lookup dnslookup clientip as ip \
| table clienthost app ip
disabled = 1

[AllSplunkLevel - Time skew on Splunk Servers]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 53 2,6,10,14,18,22 * * *
description = Chance the alert requires action? Moderate. A time skew should not exist, if we see this alert then something is not working in NTP...
dispatch.earliest_time = -4h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime","time"]
display.general.type = statistics
display.page.search.mode = fast
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("A time skew issue likely shows an issue on the endpoint forwarder rather than a Splunk server but it is useful to watch for")`\
index=_internal "A time skew of approximately"  sourcetype=splunkd \
| dedup host \
| fields _raw, host
disabled = 1

[DeploymentServer - Application Not Found On Deployment Server]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = 14 * * * *
description = Chance the alert requires action? High. The application was not found on the deployment server
dispatch.earliest_time = -1h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("This usually indicates a misconfigured serverclass.conf or a missing application from the deployment-apps directory")` \
index=_internal `deploymentserverhosts` "ERROR Serverclass - Failed to load app." sourcetype=splunkd \
| bin _time span=20m \
| top Application, path, _time
disabled = 1

[DeploymentServer - Forwarder has changed properties on phone home]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 37 6 * * 3
description = Chance the alert requires action? Moderate. Only detect when a forwarder has switched IP's or something strange has happened, ignore multiple DNS names for the same IP
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("This looks for unusual changes on the phone home to the deployment server, this alert can also be completely harmless")` \
index=_internal "has changed some of its properties on the latest phone home.Old properties are"  source="/opt/splunk/var/log/splunk/splunkd.log"  sourcetype=splunkd \
| rex "Client with Id '(?P<clientid>[^']+)" \
| sort clientid \
| eventstats count by clientid \
| where count>3 \
| stats values(ip) AS "IP List", values(dns) AS "DNS names", values(hostname) AS "Hostname List", values(uts) AS uts by name \
| eval numberOfIPs=mvcount("IP List"), numberOfHostnames=mvcount("Hostname List") \
| search `comment("Having multiple DNS names for an IP address is almost normal here, however multiple IPs or hostnames might be a real issue. Ignoring multiple DNS names only")` numberOfIPs>1 OR numberOfHostnames>1
disabled = 1

[DeploymentServer - btool validation failures occurring on deployment server]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 28 11 * * *
description = Chance the alert requires action? Moderate. Email about any btool validation errors on the deployment server
dispatch.earliest_time = -1d@d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("This alert detects when the deployment server is throwing some kind of warning about an application it is going to deploy. The exclusion list includes lines that are not really relevant as they appear later in the log entries")` \
index=_internal `deploymentserverhosts` "WARN  Application" sourcetype=splunkd \
 NOT "There were the following errors in btool check:" \
`comment("Splunk for stream doesn't include a config file which causes errors, however it appears to work without it...")` NOT "/opt/splunk/etc/deployment-apps/Splunk_TA_stream*" \
| dedup message | fields _time _raw host
disabled = 1

[ForwarderLevel - Bandwidth Throttling Occurring]
action.email = 1
action.email.inline = 1
action.email.reportServerEnabled = 0
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.suppress.period = 12h
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 37 02 * * *
description = Chance the alert requires action? High. Cases where the splunk forwarder is unable to send all the data to splunk due to maxKbps limit
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.general.type = statistics
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("This alert detects universal forwarders that have hit the maxKbps setting in the limits.conf and might need to be investigated")` \
index=_internal "has reached maxKBps. As a result, data forwarding may be throttled" sourcetype=splunkd\
| stats count(_raw) by host as countPerHost \
| where countPerHost > 1
disabled = 1

[ForwarderLevel - File Too Small to checkCRC occurring multiple times]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.reportServerEnabled = 0
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 1
alert.suppress.period = 12h
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0,15,30,45 * * * *
description = Chance the alert requires action? Low. CRC checksum errors multiple times in may indicate a problem with the crc checksum on the particular file, it's also possible we are seeing a zero sized file or a rolled file...
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.general.type = statistics
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = search
request.ui_dispatch_view = search
search = `comment("An experimental alert to detect the seekcrc too small errors in the splunkd.log file occurring a bit too regularly")` \
index=_internal "File too small to check seekcrc, probably truncated" \
sourcetype=splunkd\
`comment("Older universal forwarders have a variety of logs that will never be more than zero sized, therefore this error is legitimate for them")`\
NOT (file="'/*/splunkforwarder/var/log/splunk/license_usage.log'" OR file="'/*/splunkforwarder/var/log/splunk/license_usage_summary.log'" OR file="'/*/splunkforwarder/var/log/splunk/mongod.log'" OR file="'/*/splunkforwarder/var/log/splunk/remote_searches.log'" OR file="'/*/splunkforwarder/var/log/splunk/scheduler.log'" OR file="'/*/splunkforwarder/var/log/splunk/searchhistory.log'" OR file="'/*/splunkforwarder/var/log/splunk/splunkd_ui_access.log'" OR file="'/*/splunkforwarder/var/log/splunk/crash-*'" OR file="'/*/splunkforwarder/var/log/splunk/btool.log'" OR file="'/*/splunkforwarder/var/log/splunk/license_audit.log'")\
`comment("Older windows based universal forwarders can also have these same zero sized log files, therefore this error is legitimate for them")`\
NOT (file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\license_usage.log'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\license_usage_summary.log'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\mongod.log'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\remote_searches.log'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\scheduler.log'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\searchhistory.log'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\splunkd_ui_access.log'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\crash-*'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\btool.log'" OR file="'\\*\\SplunkUniversalForwarder\\var\\log\\splunk\\license_audit.log'")\
`comment("Splunk enterprise instances running on non-official hostnames")`\
NOT (file="'/opt/splunk/var/log/splunk/license_usage.log'" OR file="'/opt/splunk/var/log/splunk/license_usage_summary.log'" OR file="'/opt/splunk/var/log/splunk/mongod.log'" OR file="'/opt/splunk/var/log/splunk/remote_searches.log'" OR file="'/opt/splunk/var/log/splunk/scheduler.log'" OR file="'/opt/splunk/var/log/splunk/searchhistory.log'" OR file="'/opt/splunk/var/log/splunk/splunkd_ui_access.log'" OR file="'/opt/splunk/var/log/splunk/crash-*'" OR file="'/opt/splunk/var/log/splunk/btool.log'" OR file="'/opt/splunk/var/log/splunk/license_audit.log'")\
| stats sum(linecount) as numberOfEntries by host, file\
| where numberOfEntries > 10
disabled = 1

[ForwarderLevel - Forwarders in restart loop]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 1
alert.suppress.period = 60m
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0,15,30,45 * * * *
description = Chance the alert requires action? Moderate. Attempt to detect universal forwarders that are restarting too often
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("If a forwarder restarts more than 5 times in 15 minutes there might be a problematic script that is restarting it too often")` \
index=_internal "Received shutdown signal." sourcetype=splunkd\
| stats count(_raw) as restartCount by host \
| where restartCount > 5
disabled = 1

[ForwarderLevel - SSL Errors In Logs (Potential Universal Forwarder and License Issue)]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 53 22 * * *
description = Chance the alert requires action? Moderate. SSL errors from Windows forwarder sin the past have resulted in duplication and excessive license usage, this alert exists to detect this scenario. 
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","indextime","host"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Excessive SSL errors may relate to a bug in the universal forwarder, if the SSL errors relate to duplication this could cause a license usage issue")` \
index=_internal sourcetype=splunkd source=*splunkd.log NOT (`splunkenterprisehosts`)\
"sock_error = 10054. SSL Error = error:00000000:lib(0):func(0):reason(0)"\
| top limit=500 host
disabled = 1

[ForwarderLevel - Splunk Forwarder Down]
action.email = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0 * * * *
description = Chance the alert requires action? Low. Splunk Forwarders Down (excluding timeshift servers and AWS cloud forwarders)
dispatch.earliest_time = -4h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","tag::eventtype","tag","status","data_sourcetype"]
display.general.type = statistics
display.page.search.mode = verbose
display.page.search.tab = statistics
display.statistics.drilldown = row
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | metadata type=hosts index=_internal \
| search `comment("Find forwarders that have recently stopped talking to the indexers / appear to be down")`\
NOT (`splunkenterprisehosts`)\
| eval age=now()-recentTime | eval status=if(age<1200,"UP","DOWN") \
| eval "Last Active On"=strftime(recentTime, "%+") \
| rename age as Age \
| eval Hour=round(Age/3600,0)\
| eval Minute=round((Age%3600)/60,0)\
| eval Age="-".Hour."h"." : ".Minute."m" \
| table host, status, "Last Active On", Age \
| search status=DOWN \
| lookup dnslookup clienthost AS host
disabled = 1

[ForwarderLevel - Splunk HTTP Listener Overwhelmed]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = 53 3 * * *
description = Chance the alert requires action? High. HTTP listeners should not be overwhelmed with incoming connections
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","indextime","host","loglevel","search_id"]
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Find if the HTTP listener for HEC is overloaded / cannot cope with the incoming load of data")` \
index=_internal "HttpListener - Can't handle request for" source="/opt/splunk/var/log/splunk/splunkd.log" `splunkenterprisehosts`
disabled = 1

[ForwarderLevel - Splunk Heavy logging sources]
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 4,34 * * * *
description = Chance the alert requires action? Low. Sources that are sending a large amount of log data...
dispatch.earliest_time = -30m@m
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Find splunk sources sending excessive amounts of logs in and then conditionally email the right team members")` \
index=_internal source=*license_usage.log type=Usage `licensemasterhost` sourcetype=splunkd\
| stats  sum(b) as totalBytes by s, h, idx, st  \
| eval  totalMBInPast30Mins=round(totalBytes/1024/1024)  \
| where totalMBInPast30Mins>500\
| table s, h, idx, st,  totalMBInPast30Mins \
| eval email_to="updateme@company.com" | sendresults showemail=f subject="Splunk Alert: ForwarderLevel - Splunk Heavy logging sources" body="The alert condition for Splunk sources sending large amounts of logs was triggered." maxrcpts=2
disabled = 1

[ForwarderLevel - Splunk Universal Forwarders Exceeding the File Descriptor Cache]
action.email = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 0 11 * * *
description = Chance the alert requires action? Low. These forwarders may need an increase in their file descriptor cache limits
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","title","severity"]
display.general.type = statistics
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The file descriptor cache been full is a potential indicator that we are monitoring directories with many files and this might cause the forwarder to use more CPU")`\
index=_internal "TailReader - File descriptor cache is full" sourcetype=splunkd\
| stats values(message), count by host
disabled = 1

[ForwarderLevel - Splunk forwarders are having issues with sending data to indexers]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 28 * * * *
description = Chance the alert requires action? Low. A level of these alerts just mean the indexer is busy / not receiving data fast enough, many alerts indicate the indexer is having serious issues.
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","indextime","host","loglevel","search_id","serialno"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("A could not send data to output queue from the indexers or heavy forwarders often indicates a performance issue")` \
index=_internal sourcetype=splunkd source=*splunkd.log "Could not send data to output queue" `indexerhosts` OR `heavyforwarderhosts`\
| stats count by host \
| search (count>0 NOT `heavyforwarderhosts`) OR (count>5 `heavyforwarderhosts`)
disabled = 1

[ForwarderLevel - Splunk forwarders failing due to disk space issues]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = 45 * * * *
description = Chance the alert requires action? High. A universal forwarder has run out of disk space
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime","time"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Detect universal forwarders that do not have any disk space left and therefore cannot work as expected")` \
index=_internal source=/opt/splunk/splunkforwarder/var/log/splunk/splunkd.log OR source=/opt/splunkforwarder/var/log/splunk/splunkd.log "No space left on device" \
| top host
disabled = 1

[ForwarderLevel - Splunk universal forwarders with ulimit issues]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0 10 * * 1
description = Chance the alert requires action? High. Universal forwarder with ulimit issues
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
schedule_window = 50
search = `comment("Detect universal forwarders that have the ulimit set too low")` \
index=_internal log_level=WARN sourcetype=splunkd component=ulimit "Splunk may not work due to low file size limit" \
| dedup host \
| fields host _raw
disabled = 1

[ForwarderLevel - Unusual number of duplication alerts]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 07 22 * * *
description = Chance the alert requires action? Low. An unusual number of duplication alerts has appeared from these universal forwarders
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","indextime","host"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The number of warnings about duplication seems unusually high and may require investigation")` \
index=_internal sourcetype=splunkd source=*splunkd.log NOT (`splunkenterprisehosts`) "duplication" \
| stats count by host \
| where count > 50
disabled = 1

[ForwarderLevel - crcSalt or initCrcLength change may be required]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.reportServerEnabled = 0
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 1
alert.suppress.period = 12h
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 53 2 * * *
description = Chance the alert requires action? Low. The forwarder is advising the crcSalt = <SOURCE> or an initCrcLength change may be required on these files therefore these should be investigated.
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Look for issues relating to CRC salt on any files...the universal forwarder settings may need tweaking to ensure the file is read as expected, or it may be a rolled file")` \
index=_internal "You may wish to use larger initCrcLen for this sourcetype" sourcetype=splunkd \
`comment("Attempt to exclude rolled files from the check by looking for the most common pattern (.1, .2, .10 or similar)")` \
`comment("This alert aims to find files where crcSalt = <SOURCE> might be required in the inputs.conf file or a tweak to the initCrcLen...")`\
| regex file!="\.\d+$" \
| top limit=500 file, host, message
disabled = 1

[ForwarderLevel - Splunk Universal Forwarders that are time shifting]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.reportServerEnabled = 0
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 0 0 * * *
description = Chance the alert requires action? Moderate. The clock has changed many times on this server and may indicate a timeshfiting test environment
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Detect universal forwarders that appear to be moving their clocks backwards/into the past, these may need an exclusion from Splunk")` \
index=_internal log_level=ERROR OR log_level=WARN  \
"Detected system time adjusted" sourcetype=splunkd \
| where timePeriod > 100000 \
| dedup host\
| fields host, _raw
disabled = 1


[IndexerLevel - ClusterMaster Advising SearchOrRep Factor Not Met]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 1
alert.suppress.period = 3h
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = */10 * * * *
description = Chance the alert requires action? High. The cluster master shows that either not all data is searchable or rep/search factors are not met
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /services/cluster/searchhead/generation splunk_server=local | where is_searchable!=1 OR replication_factor_met!=1 OR search_factor_met!=1 | table is_searchable replication_factor_met, search_factor_met\
| search `comment("If the cluster master advises there is an issue, you probably want to check why")`
disabled = 1

[IndexerLevel - Future Dated Events that appeared in the last week]
action.email = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 0 0 * * 2
description = Chance the alert requires action? High. Search for any data that has future based time-stamping, this likely shows a date parsing issue or a server sending logs with a date in the future
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Data should not appear from the future...this alert finds that data so it can be investigated")` \
index=* earliest=+5m latest=+20y \
| eval ahead=abs(now() - _time)\
| eval indextime=_indextime\
| bin span=1d indextime \
| stats avg(ahead) as averageahead, max(_time) AS maxTime, min(_time) as minTime, count by host, sourcetype, index, indextime\
| where indextime>(now()-604800) AND averageahead > 1000\
| eval averageahead =tostring(averageahead, "duration")\
| eval indextime=strftime(indextime, "%+"), maxTime = strftime(maxTime, "%+"), minTime = strftime(minTime, "%+")
disabled = 1

[IndexerLevel - Failures To Parse Timestamp Correctly (excluding breaking issues)]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0 3 * * 5
description = Chance the alert requires action? Moderate. Failures to parse incoming log file timestamps, this excludes a timestamp failure due to the event been broken (there is a separate alert for breaking issues)
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Timestamp parsing has failed, and it doesn't appear to be related to the event been broken due to having too many lines, that is a different alert")` \
index=_internal sourcetype=splunkd "Failed to parse timestamp. Defaulting to timestamp of previous event" source="/opt/splunk/var/log/splunk/splunkd.log" `indexerhosts` OR `heavyforwarderhosts` \
| bin _time span=1m \
| rex "source::(?P<data_source>[^|]+)\|host::(?P<data_host>[^|]+)\|(?P<data_sourcetype>[^|]+)" \
| search NOT [search index=_internal sourcetype=splunkd "Breaking event because limit of " source="/opt/splunk/var/log/splunk/splunkd.log" `indexerhosts` OR `heavyforwarderhosts` \
              | bin _time span=1m \
              | table _time data_source data_host data_sourcetype] \
| top limit=10000 data_host data_source data_sourcetype
disabled = 1

[IndexerLevel - IndexConfig Warnings from Splunk indexers]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 57 2 * * 4
description = Chance the alert requires action? High. IndexConfig warnings are usually a problem so should be investigated...
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","indextime","host","loglevel","search_id","serialno"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("IndexConfig warnings are generally a problem")` \
index=_internal "WARN  IndexConfig" source="/opt/splunk/var/log/splunk/splunkd.log" `indexerhosts` | top message
disabled = 1

[IndexerLevel - Indexer Queues May Have Issues]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 1
alert.suppress.period = 1h
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = */11 * * * *
description = Chance the alert requires action? Low. One or more indexer queues have been filled for a period of time and may require investigation.
dispatch.earliest_time = -1h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("This alert is borrowed from the monitoring console. When the queues are filled there is an issue in the indexer cluster!")`\
index=_internal `indexerhosts` source=*metrics.log sourcetype=splunkd group=queue\
| eval name=case(name=="aggqueue","2 - Aggregation Queue",\
 name=="indexqueue", "4 - Indexing Queue",\
 name=="parsingqueue", "1 - Parsing Queue",\
 name=="typingqueue", "3 - Typing Queue") \
| eval ingest_pipe = if(isnotnull(ingest_pipe), ingest_pipe, "none") | search ingest_pipe=* \
| eval max=if(isnotnull(max_size_kb),max_size_kb,max_size) \
| eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size) \
| eval fill_perc=round((curr/max)*100,2) \
| bin _time span=10m\
| stats Median(fill_perc) AS "fill_percentage" by host, _time, name | where fill_percentage>2 | eventstats count(name) AS countByQueueName by name,host | where countByQueueName>2
disabled = 1

[IndexerLevel - Indexer replication queue issues to some peers]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 1
alert.suppress.period = 90m
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = */11 * * * *
description = Chance the alert requires action? Low. Indexer replication queue issues to some peers may prevent indexing of data and result in a large index queue
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.mode = fast
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("If the replication queue is full, then depending on the replication factor this can stop / slow indexing")` \
index=_internal `indexerhosts` "replication queue for " "full" sourcetype=splunkd\
| rename peer AS guid \
| join guid [|  rest /services/search/distributed/peers  | fields guid peerName]\
| bin _time span=10m \
| stats count by peerName, _time \
| where count>2
disabled = 1

[IndexerLevel - Rolling Hot Bucket Failure]
action.email = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 5
counttype = number of events
cron_schedule = 0,15,30,45 * * * *
description = Chance the alert requires action? High. Hot buckets are throwing errors while trying to roll
dispatch.earliest_time = -15m@m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("If this alert fires, we are potentially out of disk in the hot section or something else has gone wrong")` \
index=_internal "Not rolling hot buckets on further errors to this target" sourcetype=splunkd source="/opt/splunk/var/log/splunk/splunkd.log"
disabled = 1

[IndexerLevel - Splunk Indexers Losing Contact With Master]
action.email = 1
action.email.inline = 1
action.email.reportServerEnabled = 0
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0 4 * * *
description = Chance the alert requires action? Moderate. One or more splunk indexers have lost contact with the splunk cluster master server. This may require additional investigation.
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","tag::eventtype","tag","status"]
display.statistics.drilldown = row
display.visualizations.charting.chart = line
display.visualizations.show = 0
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Either the master is down or the indexers are having issues contacting the master")` \
index=_internal "master is down" sourcetype=splunkd source=/opt/splunk/var/log/splunk/splunkd.log log_level="WARN" \
| fields _time _raw host
disabled = 1

[IndexerLevel - Uneven Indexed Data Across The Indexers]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 56 1,5,9,13,17,21 * * *
description = Chance the alert requires action? Moderate. The data is not been spread across the indexers correctly during this last 4 hour block
dispatch.earliest_time = -4h@h
dispatch.latest_time = @h
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
display.visualizations.charting.chart.stackMode = stacked100
enableSched = 1
quantity = 10
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | tstats summariesonly=t count WHERE index="*" by splunk_server _time span=10m\
| search `comment("If the balance of data between indexer cluster members becomes very unbalanced then the searches tend to spend more CPU on a particular indexer / search peer and this eventually creates issues")`\
| sort _time \
| eventstats sum(count) AS totalCountForTime by _time \
| eval perc=round((count/totalCountForTime)*100,2) \
| where perc>25
disabled = 1

[IndexerLevel - Weekly Broken Events Report]
action.email = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 0 5 * * 4
description = Chance the alert requires action? Moderate. These events are been broken due to reaching the maximum number of lines limit...
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The event that came in was greater than the maximum number of lines that were configured, therefore it was broken into multiple events...")` \
index=_internal "AggregatorMiningProcessor - Breaking event because limit of" sourcetype=splunkd source="/opt/splunk/var/log/splunk/splunkd.log" \
| rex "Breaking event because limit of (?P<curlimit>\d+)" \
| top limit=500 data_sourcetype curlimit data_host
disabled = 1

[IndexerLevel - Weekly Truncated Logs Report]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 0 5 * * 2
description = Chance the alert requires action? Moderate. These events are been truncated due to hitting the truncation limit
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The line was truncated eu to length, this might need adjustment")` \
index=_internal "Truncating line because limit of" sourcetype=splunkd source="/opt/splunk/var/log/splunk/splunkd.log" \
| rex "Truncating line because limit of (?P<curlimit>\d+) bytes.*with a line length >= (?P<approxlinelength>\S+)" \
| stats count, avg(approxlinelength) AS avgApproxLineLength, max(approxlinelength) AS maxApproxLineLength by data_sourcetype, curlimit\
| eval avgApproxLineLength = round(avgApproxLineLength)\
| sort - count
disabled = 1

[IndexerLevel - Valid Timestamp Invalid Parsed Time]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0 2 * * 3
description = Chance the alert requires action? Moderate. The timestamp was parsed but an error was thrown to advise that the timestamp does not appear to be correct
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The timestamp parsing did run but the timestamp found did not match previous events so the time parsing may need a review")`\
index=_internal sourcetype=splunkd source=/opt/splunk/var/log/splunk/splunkd.log `indexerhosts` OR `heavyforwarderhosts` \
"outside of the acceptable time window. If this timestamp is correct, consider adjusting" \
OR "is too far away from the previous event's time" \
OR "is suspiciously far away from the previous event's time"\
| rex "source::(?P<data_source>[^|]+)\|host::(?P<data_host>[^|]+)\|(?P<data_sourcetype>[^|]+)" \
| top limit=10000 data_host data_source data_sourcetype
disabled = 1

[SearchHeadLevel - Long Running Searches Found]
action.analyzeioc.param.verbose = 0
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 38 0,4,8,12,14,16,20 * * *
description = Chance the alert requires action? Low. Extra long running searches have been found
dispatch.earliest_time = -4h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime","time"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Find any search running longer than a period of time, this is useful for tracking down poorly built search queries or dashboards")` \
index=_audit sourcetype=audittrail  \
`splunkenterprisehosts` \
`comment("Exclude accelerated searches")` \
savedsearch_name!=_ACCELERATE*\
total_run_time>300\
`comment("Exclude various standard/expected searches")` \
savedsearch_name!="Generate Meta Woot! every 15 mins" savedsearch_name!="Generate NMON*"\
`comment("At this point we have a list of searches minus the various exclusions, we now filter out the real time searches as they will always run for a long period of time...")`\
| regex search_id!="rt.*" | table savedsearch_name, search_id, total_run_time, search_et, search_lt, api_et, api_lt, scan_count, _time, user, info, host | eval search_et=strftime(search_et, "%d/%m/%Y %H:%M"), search_lt=strftime(search_lt, "%d/%m/%Y %H:%M"), api_et=strftime(api_et, "%d/%m/%Y %H:%M"), api_lt=strftime(api_lt, "%d/%m/%Y %H:%M"), _time=strftime(_time, "%d/%m/%Y %H:%M")
disabled = 1

[SearchHeadLevel - Realtime Scheduled Searches are in use]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.message.report = The scheduled report '$name$' has run. Please fix the searches listed
action.email.reportServerEnabled = 0
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 8 0,4,8,12,16,20 * * *
description = Chance the alert requires action? High. Realtime searches should not be scheduled
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /servicesNS/-/-/saved/searches \
| search `comment("Find realtime scheduled searches, they should not be enabled")`\
| table title author, realtime_schedule, cron_schedule, description, disabled, dispatch.earliest_time, dispatch.index_earliest, dispatch.index_latest, dispatch.latest_time, dispatchAs, eai:acl.app, eai:acl.owner, eai:acl.owner, updated, qualifiedSearch, is_scheduled, next_scheduled_time, alert_type, schedule_priority\
| search dispatch.earliest_time=rt* next_scheduled_time!=""
disabled = 1

[SearchHeadLevel - Scheduled Searches That Cannot Run]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 16 10 * * *
description = Chance the alert requires action? High. As found in the DMC console, moving it into an alert so we can get alerted to the problem rather than checking a dashboard/log about this
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("These searches are scheduled but for some reason cannot run (eg. invalid search syntax)")`\
index=_internal `searchheadhosts` sourcetype=scheduler\
| stats max(_time) AS mostRecentlySeen, values(status) AS status by message, savedsearch_id, log_level\
| eventstats count(eval(status="success")) AS successCount, count(eval(searchmatch("log_level=WARN OR log_level=ERROR"))) AS warnerrorcount by savedsearch_id\
| where warnerrorcount>0 AND successCount<2\
| cluster t=0.7 field=message showcount=t countfield=count\
| rex field=savedsearch_id "^(?P<user>[^;]+)"\
| sort - count\
| rename message as Message, count as Count \
| eval mostRecentlySeen = strftime(mostRecentlySeen, "%+")\
| fields - cluster_label
disabled = 1

[SearchHeadLevel - Scheduled Searches without a configured earliest and latest time]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.message.report = The scheduled report '$name$' has run. Please fix the searches listed
action.email.reportServerEnabled = 0
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 1
alert.suppress.period = 8h
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 8 0,4,8,12,16,20 * * *
description = Chance the alert requires action? High. A scheduled search without time limits could kill the Splunk indexers with CPU / IO issues depending on the criteria of the search
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /servicesNS/-/-/saved/searches \
| search `comment("Find scheduled searches where they are searching over all time, this is generally not good practice and can cause performance issues")`\
dispatch.earliest_time="" OR dispatch.earliest_time="0" next_scheduled_time!=""\
`comment("Application specific exclusions")` \
 eai:acl.app!="splunk_archiver" NOT ((title="Config: Topology Playback Appender" OR title="Config: Topology History Appender" OR title="AWS Billing - Account Name" OR title="App Upgrader" OR title="Machine Learning: Recommendation") eai:acl.app="splunk_app_aws") title!="Generate NMON Inventory Lookup Table" \
 | table title author, alert_type, realtime_schedule, cron_schedule, description, disabled, dispatch.earliest_time, dispatch.index_earliest, dispatch.index_latest, dispatch.latest_time, dispatchAs, eai:acl.app, eai:acl.owner, eai:acl.owner, updated, schedule_window, schedule_priority, qualifiedSearch, is_scheduled, is_visible, max_concurrent, next_scheduled_time, splunk_server \
 | regex qualifiedSearch!="^\s*(rest|tstats|noop|inputlookup|inputthreatlist|from inputlookup:)" \
 | rex field=qualifiedSearch "earliest=(?P<earliestTime>\S+)"\
 | where isnull(earliestTime) \
 | fields - alert_type, dispatchAs, schedule_window, schedgule_priority, max_concurrent, dispatch.index_earliest, dispatch.index_latest, earliestTime
disabled = 1

[SearchHeadLevel - Scheduled searches not specifying an index]
action.analyzeioc.param.verbose = 0
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.message.report = The scheduled report '$name$' has run. Please fix the searches listed
action.email.reportServerEnabled = 0
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 14 6 * * 1-5
description = Chance the alert requires action? High. These searches are either using index=* or not specifying an index at all and relying on the default set of indexes.
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /servicesNS/-/-/saved/searches\
| search `comment("Look over all scheduled searches and find those not specifying/narrowing down to an index, or using the index=* trick")`\
| table title author, alert_type, realtime_schedule, cron_schedule, description, disabled, dispatch.earliest_time, dispatch.index_earliest, dispatch.index_latest, dispatch.latest_time, dispatchAs, eai:acl.app, eai:acl.owner, eai:acl.owner, updated, schedule_window, schedgule_priority, qualifiedSearch, is_scheduled, is_visible, max_concurrent, next_scheduled_time, splunk_server \
| search next_scheduled_time!="" eai:acl.app!="splunk_archiver" eai:acl.app!="splunk_app_windows_infrastructure" eai:acl.app!="splunk_app_aws" eai:acl.app!="nmon" \
| regex qualifiedSearch!=".*index\s*(!?)=\s*([^*]|\*\S+)" \
| regex qualifiedSearch!="^\s*(rest|tstats|`ts_tstats_all`|`ts_tstats_url_all`|noop|inputlookup|savedsearch |dbinspect)" \
| fields - alert_type, dispatchAs, schedule_window, schedgule_priority, max_concurrent, dispatch.index_earliest, dispatch.index_latest
disabled = 1

[SearchHeadLevel - Script failures in the last day]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 0 22 * * *
description = Chance the alert requires action? Moderate. Scripts are throwing errors which may indicate an issue
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Shell scripts running from Splunk are failing to run or throwing errors")` \
index=_internal sourcetype=splunkd command="runshellscript" log_level="ERROR" OR log_level="WARN" | fields _time, host, _raw
disabled = 1

[SearchHeadLevel - Splunk Max Historic Search Limits Reached]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 0 11 * * *
description = Chance the alert requires action? Moderate. Splunk Max Historic Search Limits Reached
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Once the max historic search limit has been reached the search jobs will be queued, this can be an issue if the limit is set too low (or too high)")` \
index=_internal `splunkenterprisehosts` "The maximum number of historical concurrent system-wide searches has been reached" OR "The system is approaching the maximum number of historical searches that can be run concurrently" source="/opt/splunk/var/log/splunk/splunkd.log" \
| fields _time, host
disabled = 1

[SearchHeadLevel - Splunk Scheduler Skipped Searches and the reason]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 57 2,6,10,14,18,22 * * *
description = Chance the alert requires action? Low. Provides the skipped searches and a list of reasons why they were skipped
dispatch.earliest_time = -4h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("For some reason this search had to be skipped, this might be due to over scheduling the search or an inefficient search or similar")` \
index=_internal sourcetype=scheduler status=skipped `splunkenterprisehosts` \
| stats count, earliest(_time) AS firstSeen, latest(_time) AS lastSeen by savedsearch_id, reason, app, concurrency_category, concurrency_context, concurrency_limit, search_type, user, host\ 
| eval firstSeen = strftime(firstSeen, "%+"), lastSeen=strftime(lastSeen, "%+")
disabled = 1

[SearchHeadLevel - Splunk Users Violating the Search Quota]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 0 3 * * *
description = Chance the alert requires action? Low. These users have reached the search quota but may not be aware of this issue.
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","indextime","host","loglevel","search_id"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The listed users have reached the search quota and may need to be informed of this, or they may need to be added to an ignore list for this alert")`\
index=_internal `searchheadhosts` source="/opt/splunk/var/log/splunk/splunkd.log" "was previously reported to be hung but has completed"\
| rex "Queued job id\s+=\s+(?P<username>[^_]+)"\
| bin span=24h _time \
| top limit=200 username, reason, host, _time, provenance \
| where count>10
disabled = 1

[SearchHeadLevel - Users exceeding the disk quota]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 0 2 * * *
description = Chance the alert requires action? High. One or more users have reached the disk quota limit and may not be aware of this...
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","indextime","host","loglevel","search_id"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The listed users have reached the maximum disk quota, they may be unaware so it is best to let them know about this issue...")` \
index=_internal sourcetype=splunkd `splunkenterprisehosts` source="/opt/splunk/var/log/splunk/splunkd.log" "maximum disk usage quota"\
| bin span=4h _time \
| top username, _time, reason, host
disabled = 1

[AllSplunkLevel execprocessor errors]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 28 5 * * *
description = Chance the alert requires action? Low. This alert can be very noisy, this will return any execprocessor errors from any script on any Splunk server!
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("An attempt to find the execprocessor errors these are often scripts or application which are having some kind of issue...")`\
index=_internal "ERROR ExecProcessor" sourcetype=splunkd NOT "Ignoring: \"" \
| dedup message, host | fields host _raw
disabled = 1

[IndexerLevel - Time format has changed multiple log types in one sourcetype]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0 2 * * 1
description = Chance the alert requires action? High. A changing time format is likely due to multiple log types using the same sourcetype or a date time parsing issue
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("This search detects when the time format has changed within the files 1 or more times, the time format per sourcetype should be consistent")`\
index=_internal "DateParserVerbose - Accepted time format has changed" sourcetype=splunkd source=/opt/splunk/var/log/splunk/splunkd.log `indexerhosts` OR `heavyforwarderhosts` \
| rex "source::(?P<source>[^|]+)\|host::(?P<host>[^|]+)\|(?P<sourcetype>[^|]+)" \
| fields host _raw
disabled = 1

[IndexerLevel - Volume (Cold) Has Been Exceeded]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 54 1,5,9,13,17,21 * * *
description = Chance the alert requires action? High. The non-hot volume has been exceeded therefore we are deleting data before the time limit is hit...
dispatch.earliest_time = -5h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The cold volume is causing indexes to be trimmed, this may or may not be an issue...")` \
index=_internal `indexerhosts` sourcetype=splunkd source="/opt/splunk/var/log/splunk/splunkd.log" "Size exceeds max, will have to trim " volume!="hot Size exceeds max"\
| fields host, _raw
disabled = 1

[SearchHead Level - Splunk Scheduler excessive delays in executing search]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 57 6 * * 2
description = Chance the alert requires action? Moderate. Long latency delays in scheduled searches may indicate an issue, however the scheduled time of the search is what determine the search window. Therefore this only shows when it has taken a long period of time to execute an actual search (there is another alert for skipped searches)
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("These searches were scheduled to run at a particular time but the actual run time was more than X seconds later, which might indicate a search head performance issue")` \
index=_internal `splunkenterprisehosts` sourcetype=scheduler app=* scheduled_time=* \
| eval time=strftime(_time,"%Y-%m-%d %H:%M:%S") \
| eval delay_in_start = (dispatch_time - scheduled_time) \
| where delay_in_start>100\
| eval scheduled_time=strftime(scheduled_time,"%Y-%m-%d %H:%M:%S") \
| eval dispatch_time=strftime(dispatch_time,"%Y-%m-%d %H:%M:%S") \
| rename time AS endTime \
| table host,savedsearch_name,delay_in_start, scheduled_time, dispatch_time, endTime, run_time, status, user, app \
| sort -delay_in_start \
| dedup host,savedsearch_name,delay_in_start
disabled = 1

[SearchHeadLevel - Splunk login attempts from users that do not have any LDAP roles]
action.analyzeioc.param.verbose = 0
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 0 8 * * *
description = Chance the alert requires action? High. These usernames have appeared in the logs but they have no mapped roles
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The listed users have attempted to login but were unable to, it is likely they do not have any LDAP role yet and should be informed of this")` \
index=_internal `searchheadhosts` "Couldn't find matching groups for user" OR  "but none are mapped to Splunk roles" OR "SSO failed - User does not exist" source="/opt/splunk/var/log/splunk/splunkd.log"\
| join user [search index=_internal `searchheadhosts` action=login status=failure reason=user-initiated OR reason=sso-failed] \
| dedup user \
| table _time, user, host
disabled = 1

[IndexerLevel - Buckets are been frozen due to index sizing]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = 33 3,7,11,15,19,23 * * *
description = Chance the alert requires action? High. One or more indexes have hit the index size limit and are now been frozen due to this
dispatch.earliest_time = -5h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The indexer is freezing buckets due to disk space pressure before the frozenTimePeriodInSecs limit has been reached, this could be a problem if it is not expected...")`\
`comment("_introspection defaults to size based so exclude it")` \
index=_internal `indexerhosts` sourcetype=splunkd source="/opt/splunk/var/log/splunk/splunkd.log" "BucketMover - will attempt to freeze" NOT "because frozenTimePeriodInSecs=" \
bkt!="'/opt/splunk/var/lib/cold/*/_introspection/*"\
| rex field=bkt "(rb_|db_)(?P<newestDataInBucket>\d+)_(?P<oldestDataInBucket>\d+)"\
| eval newestDataInBucket=strftime(newestDataInBucket, "%+"), oldestDataInBucket = strftime(oldestDataInBucket, "%+") \
| table message, oldestDataInBucket, newestDataInBucket
disabled = 1

[IndexerLevel - Indexer Out Of Disk Space]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 5
counttype = number of events
cron_schedule = 4,19,34,49 * * * *
description = Chance the alert requires action? High. The indexer has run out of disk space while attempting to write to the filesystem
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The indexer has run out of disk space, this requires immediate investigation...")` \
index=_internal "event=onFileWritten err=\"disk out of space\"" OR "event=replicationData status=failed err=\"onFileWritten failed\"" `indexerhosts` source="/opt/splunk/var/log/splunk/splunkd.log" sourcetype=splunkd \
| top host
disabled = 1

[AllSplunkEnterpriseLevel - Core Dumps Disabled]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 27 7 * * *
description = Chance the alert requires action? Moderate. Core Dumps are disabled and this may make support cases more difficult as sometimes the core dump is required for troubleshooting purposes.
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Core dumps are disabled, if a crash occurs the Splunk Support team might not be able to assist without the core dump")`\
`comment("https://answers.splunk.com/answers/223838/why-are-my-ulimits-settings-not-being-respected-on.html applies to core limits so if the server has been rebooted the init.d may need a ulimit -Hc/Sc setting for this as well...")`\
index=_internal "WARN  ulimit - Core file generation disabled" `splunkenterprisehosts` sourcetype=splunkd source=/opt/splunk/var/log/splunk/splunkd.log \
| stats max(_time) AS mostRecentlySeen by host\
| eval mostRecentlySeen = strftime(mostRecentlySeen, "%+")
disabled = 1

[IndexerLevel - Unable to replicate thawed directories in a cluster]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 41 2,6,10,14,18,22 * * *
description = Chance the alert requires action? High. This doesn't work as documented/expected so warn about it if someone does thaw buckets to the thawed directory.
dispatch.earliest_time = -4h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Using the thaweddb location in a clustered environment might not work as expected, warn about this issue")` \
index=_internal `indexerhosts`  source="/opt/splunk/var/log/splunk/splunkd.log" sourcetype=splunkd "Failed to trigger replication" bucketType=*thaweddb* \
| rex field=bucketType "(?P<indexpath>/opt/splunk/var/lib/cold/[^/]+/[^/]+/[^/]+)/"\
| eval error="As per https://answers.splunk.com/answers/153341/thawed-buckets-error-clusterslavebuckethandler-failed-to-trigger-replication.html , as of 6.5.2 this remains a bug and you need to use the workaround in Splunk Answers..."\
| top host, error, indexpath\
| fields - count, percent
disabled = 1

[ForwarderLevel - Splunk Insufficient Permissions to Read Files]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.reportServerEnabled = 0
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 51 6 * * *
description = Chance the alert requires action? Low. An insufficient permissions to read files error was thrown...
dispatch.earliest_time = -1d@d
dispatch.latest_time = @d
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.mode = fast
display.page.search.patterns.sensitivity = 0.3
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
display.visualizations.show = 0
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | tstats count groupby source, host\
| append \
[search \
  `comment("This search looks for insufficient permissions errors, the problem here is that we might have insufficient permissions to read a file but we might later obtain the correct permissions and then read the file (as permissions changes can happen *after* the file creation...this is why there is both a tstats listing all files (only done because I cannot find a nicer way to do this, map is possibly more compute intensive), and then a search for files")`\
  index=_internal "Insufficient permissions to read file" sourcetype=splunkd | top limit=30000 file, host | rex field=file "'(?P<source>[^']+)'" | eval insufficientpermissions="true" | fields source, host, insufficientpermissions]\
| eventstats count by host, source\
| search insufficientpermissions="true" count<2\
  `comment("At this point if we see an insufficient permissions line, and we cannot see a result from the tstats showing indexed data from that file, then we have an issue, if not there is no issue with permisisons!")` \
  `comment("Insufficient permissions to read file + hint: No such file or directory when the file exists on a Splunk enterprise instance might require TAILING_SKIP_READ_CHECK = 1 in the splunk-launch.conf refer to splunk support for more info")`\
  | fields host, source\
  | search source!=*.tmp
disabled = 1

[AllSplunkLevel - TCP Output Processor has paused the data flow]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 44 * * * *
description = Chance the alert requires action? Low. A potential indicator of poor index performance or an overloaded forwarder
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("A paused TCP output processor is a potential indicator of an index performance issue, you may wish to ignore the shorter pause times such as 10 seconds if this is creating too  many alerts...")`\
index=_internal "The TCP output processor has paused the data flow. Forwarding to output group allIndexers has been blocked for"  sourcetype=splunkd source="/opt/splunk/var/log/splunk/splunkd.log" | fields host _raw | head 100
disabled = 1

[IndexerLevel - These Indexes Are Approaching The warmDBCount limit]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 48 6 * * 1
description = Chance the alert requires action? Moderate. Buckets are either now rolling or will roll to cold due to the bucket count limit in warm been reached, this may need ajustment
dispatch.earliest_time = -90d@d
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","indextime","host","loglevel"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | dbinspect index=*  state=warm\
| search `comment("Once the warmdb bucket count is reached then the buckets are moved to cold, this may be an issue if incorrectly configured, this alert warns in advance if we get close to the limit")`\
`comment("This might be a bug in 6.5.2 but the buckets are printed twice by dbinspect in some cases...")`\
| dedup bucketId, splunk_server\
| stats count AS theCount by index, splunk_server\
| stats avg(theCount) AS averageCount, max(theCount) AS maxCount, min(theCount) AS minCount, values(splunk_server) by index\
| eval averageCount = round(averageCount)\
| join index [| rest /services/data/indexes \
              | dedup title \
              | rename title AS index \
              | table index, maxWarmDBCount]\
| eval percUsed = (100/maxWarmDBCount)*averageCount\
| where percUsed > 80
disabled = 1

[SearchHeadLevel - Splunk alert failure]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 14 4 * * *
description = Chance the alert requires action? Moderate. At least one of the alert actions is failing for some alerts and this may require investigation.
dispatch.earliest_time = -1d@d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","cluster_count"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("If this alert fires you probably want to try running the search: \
 index=_internal sourcetype=splunkd source=/opt/splunk/var/log/splunk/splunkd.log `searchheadhosts` "Error in 'sendalert' command: Alert script returned error code"  OR "Invoking modular alert action=notable" OR STDERR\
 To assist is identifying the failed modular alert...\
")`\
index=_internal sourcetype=splunkd source="/opt/splunk/var/log/splunk/splunkd.log" `searchheadhosts`  "Error in 'sendalert' command: Alert script returned error code" \
| cluster showcount=true | table host, _raw
disabled = 1

[ForwarderLevel - SplunkStream Errors]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 9 11 * * *
description = Chance the alert requires action? Low. Errors from the Splunk stream forwarders
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = index=_internal source="/opt/splunkforwarder/var/log/splunk/streamfwd.log" ERROR OR FATAL  | cluster showcount=true  | fields host _raw \
| append [search index=_internal `searchheadhosts` sourcetype=splunkd source=/opt/splunk/var/log/splunk/splunkd.log "Shutdown complete in" ]\
| fields host, _raw
disabled = 1

[SearchHeadLevel - LDAP users have been disabled or left the company cleanup required]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 33 11 * * *
description = Chance the alert requires action? High. These users have been disabled or left the company but their users files are on the filesystem and this is therefore triggering warning or errors in the Splunk logs, please cleanup the old user files for these users.\
A separate alert should exist for orphaned searches...
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("If we see a failed to get LDAP user 'username' from any configured servers then that is a sign the user is no longer in the company. However if there is also a message of Couldn't find matching groups for that same user it is more likely that they exist but just do not have access to Splunk")`\
`comment("If you see this alert fire, than you probably need to cleanup the /opt/splunk/etc/users/... directory on each search head due to a user leaving/becoming disabled in LDAP. Alternatively they have a savedsearch/dashboard that you can find in the .meta files on the search head(s)")`\
index=_internal `searchheadhosts` "Failed to get LDAP user=\"" OR "Couldn't find matching groups for user=" OR "HTTPAuthManager - SSO failed - User does not exist" sourcetype=splunkd \
| dedup message \
| rex "SSO failed - User does not exist: (?P<user>\S+)"\
| stats count, values(message) AS messages, values(component), AS components values(log_level), max(_time) AS lastSeen by user, host\
| search `comment("count=1 eliminates users who are failing to login...if a user is active in LDAP but fails to login we should not not get a 'Couldn't find matching groups for user' line in the logs")`\
`comment("If we are using a single sign on system and a user without any groups attempts sign on we should see the SSO failed - User does not exist: <username> message")`\
| where user!="undefined" AND user!="nobody" AND like(messages,"Failed to get LDAP user%") AND NOT like(messages,"SSO failed - User does not exist%")\
| table user, messages, lastSeen, host\
| eval lastSeen=strftime(lastSeen, "%+")
disabled = 1

[AllSplunkLevel - DeploymentServer Application Installation Error]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 11 * * * *
description = Chance the alert requires action? High. The deployment server sent out a new application but for some reason it has failed to install
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime","index","aws_account_id"]
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = index=_internal sourcetype=splunkd source=*splunkd.log "ERROR DeployedServerclass - name=* Failed to install" OR "DeployedApplication - Installing app="\
| eventstats count(eval(log_level="ERROR")) AS errorCount, count(eval(log_level="INFO")) AS successCount by host, app \
| where errorCount>0 AND successCount<1
disabled = 1

[AllSplunkLevel - Unable To Distribute to Peer]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = 24 * * * *
description = Chance the alert requires action? Low. A Splunk instance is advising that it cannot distribute to a peer node (indexer, another search head in the cluster or similar)
dispatch.earliest_time = -60m@m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Unable to distribute to peer messages often indicate downtime or serious performance issues")`\
index=_internal "Unable to distribute to peer named" sourcetype=splunkd `splunkenterprisehosts`\
| rex "(?P<message>Unable to distribute to peer named \S+)" \
| bin _time span=10m \
| stats count by _time, message \
| where count>1
disabled = 1

[SearchHeadLevel - Alerts that have not fired an action in X days]
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.track = 0
description = A report that can be run to determine which alerts have not sent an alert based on the time period / amount of internal logs available...
dispatch.earliest_time = -30d@d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime","index","aws_account_id"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /servicesNS/-/-/saved/searches | search `comment("Attempt to find alerts which are scheduled but have not fired an action in over X days (depending on the time picker choice. Note that this report is quite expensive to run!.")`\
`comment("Eliminate summary_index as an action as that is not something the scheduler fires, and ensure this is a scheduled search, if not attempt to find any results where the saved search has done something")`\
actions!="summary_index" actions!="" next_scheduled_time!="" search!="| noop"\
| table title \
| rename title as savedsearch_name\
| join type=outer savedsearch_name [search index=_internal source="/opt/splunk/var/log/splunk/scheduler.log" sourcetype=scheduler alert_actions!="" \
| stats count by savedsearch_name] \
| fillnull\
| where count=0

[SearchHeadLevel - Data Model Acceleration Completion Status]
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.track = 0
description = The % complete of the data model which is stored on the indexer level but run from the search head level...
dispatch.earliest_time = @d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.timeRangePicker.show = 0
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
display.visualizations.show = 0
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /services/admin/summarization by_tstats=t splunk_server=local count=0 \
| search `comment("Found on https://answers.splunk.com/answers/555005/how-to-check-the-percent-of-the-dm-acceleration-co.html ")`\
| eval datamodel=replace('summary.id',"DM_".'eai:acl.app'."_","") \
| join type=left datamodel \
  [| rest /services/data/models splunk_server=local count=0 \
  | table title acceleration.cron_schedule eai:digest \
  | rename title as datamodel \
  | rename acceleration.cron_schedule AS cron] \
| table datamodel eai:acl.app summary.access_time summary.is_inprogress summary.size summary.latest_time summary.complete summary.buckets_size summary.buckets cron summary.last_error summary.time_range summary.id summary.mod_time eai:digest summary.earliest_time summary.last_sid summary.access_count \
| rename summary.id AS summary_id, summary.time_range AS retention, summary.earliest_time as earliest, summary.latest_time as latest, eai:digest as digest \
| rename summary.* AS *, eai:acl.* AS * \
| sort datamodel

[SearchHeadLevel - User - Dashboards searching all indexes]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = 30 5 * * 1-5
description = Chance the alert requires action? High. All dashboard panels that do not have an index= setting or use index=* are highlighted by this alert
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /servicesNS/-/-/data/ui/views \
| search `comment("A dashboard searching all indexes is an issue just like a scheduled search querying all indexes or using the index=* trick")` \
eai:data=*query* NOT (eai:appName=simple_xml_examples eai:acl.sharing=app)  NOT (eai:appName=nmon eai:acl.sharing=app)\
| regex eai:data="<search.*" \
| rex field=eai:data "(?P<theSearch><search(?!String)[^>]*>[^<]*<query>.*?)<\/query>" max_match=200 \
| mvexpand theSearch \
| rex field=theSearch "<search(?P<searchInfo>[^>]*)>[^<]*<query>(?P<theQuery>.*)" \
| search `comment("If we are seeing post process search then we don't want to check if it has index= because that is likely only in the base query. These are also various exclusions for legitimate searches that will not involve scanning all indexes, such as rest or a savedsearch or similar")` \
searchInfo!="*base*"\
| table searchInfo, theQuery, eai:appName, eai:acl.owner, eai:acl.sharing, label, splunk_server, title\
| regex theQuery!="index\s*=(?!\s*\*)" \
| regex theQuery!="^(\()?\s*(`|\|\s*`|\|\s*savedsearch|\| stats c(ount)?\s*\||\|\s*rest |\|\s*pivot |\|\s*inputlookup |\|\s*tstats |eventtype=|\|\s*ldapsearch |\|\s*metadata |<!\[CDATA\[ \| rest|\|\s*datamodel|\|\s*from datamodel:|\|\s*from inputlookup:|\|\s*esconfighealth)"
disabled = 1

[SearchHeadLevel - Scheduled Searches Configured with incorrect sharing]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.message.report = The scheduled report '$name$' has run. Please fix the searches listed
action.email.reportServerEnabled = 0
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0 5 * * *
description = Chance the alert requires action? High. These searches are triggering scripts or alerts which will provide a results link to Splunk. But the sharing is not app or global and therefore the link is unusable to anyone who is not the owner...
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest /servicesNS/-/-/saved/searches \
| search `comment("The problem with alerts that are configured privately is that no one beyond the author can use the results link and non-admins cannot even see the alert in Splunk!")`\
`comment("Therefore we find anything that emails that is not shared correctly *and* anything that uses a script as often the script will include a results link.")`\
`comment("The idea here is to let the end user know so they can share it appropriately, the noop search is excluded to remove scheduled views from this list")`\
is_scheduled=1 disabled=0 eai:acl.sharing!="global" eai:acl.sharing!="app" search!="| noop" actions!=""\
| eval numberOfEmailed = mvcount(split('action.email.to',"@"))-1\
| table title, eai:acl.app, author, eai:acl.sharing, actions, action.email.to, numberOfEmailed\
| where numberOfEmailed>1 OR isnull(numberOfEmailed)\
| sort author
disabled = 1

[SearchHeadLevel - Realtime Search Queries in dashboards]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@example.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 0
counttype = number of events
cron_schedule = 11 4 * * 1
description = Chance the alert requires action? High. Just a summary of all dashboards that use realtime searching...
dispatch.earliest_time = -48h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | rest/servicesNS/-/-/data/ui/views  | regex eai:data="<(earliest|latest)(Time)?>rt"\
| search `comment("Shows realtime search usage within dashboards")` eai:data=*query* NOT (eai:appName=simple_xml_examples eai:acl.sharing=app)  NOT (eai:appName=nmon eai:acl.sharing=app)\
| regex eai:data="<search.*"\
| rex field=eai:data "(?P<theSearch><search(?!String)[^>]*>[^<]*<query>.*?)<\/query>" max_match=200 \
| mvexpand theSearch \
| rex field=theSearch "<search(?P<searchInfo>[^>]*)>[^<]*<query>(?P<theQuery>.*)" \
| search searchInfo!="*base*" `comment("Exclude queries which have a base, in general they will not have a earliest/latesttime so this gets confusing")`\
`comment("It might be possible to use mvzip / mvexpand or mvindex to match the correct earliesttime/latesttime with each search query but it proved extremely difficult. So just keeping this as-is as the dashboard needs to be reviewed if it has too many realtime searches anyway")`\
| rex field=eai:data "<earliest(Time)?>(?P<earliesttime>[^<]+)" max_match=200 \
| rex field=eai:data "<latest(Time)?>(?P<latesttime>[^<]+)" max_match=200 \
| table title, eai:appName, searchInfo, theQuery,eai:acl.owner, eai:acl.sharing, label, earliesttime, latesttime, splunk_server
disabled = 1

[AllSplunkEnterpriseLevel - Transparent Huge Pages is enabled and should not be]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = 14 2 * * *
description = Chance the alert requires action? High. Transparent huge pages should never be enabled on a Splunk enterprise server as per the http://docs.splunk.com/Documentation/Splunk/latest/Installation/Systemrequirements
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.events.fields = ["source","sourcetype","indextime","host","loglevel","search_id","serialno"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Detect when transparent huge pages is enabled on a Linux server, it should be disabled.")`\
`comment("Redhat Linux has an issue where the transparent huge pages setting changes after Splunk starts if the server was rebooted, check /sys/kernel/mm/transparent_hugepage to confirm...")`\
`comment("| rest /services/server/sysinfo is an alternative if you want the current search head + indexers, but this will ignore other search heads...")``\
index=_internal "Linux transparent hugepage support, enabled=" sourcetype=splunkd `splunkenterprisehosts` enabled!="never"\
| eval error="This configuration of transparent hugepages is known to cause serious runtime problems with Splunk. Typical symptoms include generally reduced performance and catastrophic breakdown in system resp\
onsiveness under high memory pressure. Please fix by setting the values for transparent huge pages to \"madvise\" or preferably \"never\" via sysctl, kernel boot parameters, or other method recommended by your Linux distribution."\
| table _time, host, _raw, error
disabled = 1

[IndexerLevel - Old data appearing in Splunk indexes]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = 33 7 * * 0
description = Chance the alert requires action? Moderate. A slightly more complex alert that attempts to find recently indexed data that is been indexed with older timestamps, an attempt to find invalid date parsing for Splunk inputs
dispatch.earliest_time = 0
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | tstats max(_time) AS maxtime, max(_indextime) AS maxindextime, count \
where _index_earliest=-14d, earliest=-2600d, latest=-60d, host!=lbr00*.env* \
groupby source, sourcetype, index, host\
| search `comment("Find data that appears to be logged in the past, this may indicate poor timestamp parsing (or we're just ingesting really old data")`\
| eval maxtime=strftime(maxtime, "%+"), maxindextime=strftime(maxindextime, "%+")  | sort index, host, sourcetype\
| table index, source, sourcetype, host, maxtime, maxindextime, count
disabled = 1

[AllSplunkLevel - Splunk forwarders that are not talking to the deployment server]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
auto_summarize.dispatch.earliest_time = -1d@h
counttype = number of events
cron_schedule = 0 8 * * *
description = Chance the alert requires action? Moderate. All forwarders should talk to the deployment server unless they have a special reason for an exclusion...
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | tstats count where index=_internal groupby host \
| fields host \
| search `comment("This is an attempt to find any universal forwarders that send data into the indexers but do not phone home to the expected deployment server")` \
| table host \
| search NOT [search index=_internal `deploymentserverhosts` source="/opt/splunk/var/log/splunk/splunkd_access.log" sourcetype=splunkd_access \
             | rex field=uri "/services/broker/phonehome/connection_[^_]+_[89][0-9]{3}_[^_]+(_[0-9][^_]+)?_(?P<hostname>[^_]+)_" \
             | eval host=replace(hostname, ".auiag.corp", "") \
             | dedup host \
             | table host] \
| search NOT (`splunkenterprisehosts`) \
| lookup dnslookup clienthost AS host \
| search clientip!=''
disabled = 1

[AllSplunkEnterpriseLevel - sendmodalert errors]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = */15 * * * *
description = Chance the alert requires action? Low. sendmodalert errors from Splunk might advise of a failure in an alert action
disabled = 1
dispatch.earliest_time = -20m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("sendmodalert or sendalert errors and warnings may be an issue relating to the creation of alerts via a script")`\
index=_internal `splunkenterprisehosts` "ERROR sendmodalert - action=" OR "WARN sendmodalert - action" OR "Error in 'sendalert' command" \
`comment("If you need more context on the above errors add this snippet into the above search (remove the \\ and include the OR): OR \"sendmodalert - Invoking modular alert action\"")`\
`comment("We look for the sendalert commands to provide context around the errors where possible. Since notable/risks fail more often they are removed from this particular alert")`\
NOT action=notable NOT action=risk sourcetype=splunkd NOT (" - INFO]" OR "Results Link" OR "Alert Name:")\
| join sid type=outer [search index=_internal source="/opt/splunk/var/log/splunk/scheduler.log" sourcetype=scheduler | table sid, savedsearch_name, app, user]\
| table host, savedsearch_name, app, user, _raw
disabled = 1

[IndexerLevel - Indexer not accepting TCP Connections]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 3
counttype = number of events
cron_schedule = 3,18,33,48 * * * *
description = Chance the alert requires action? Low. The indexer is either overloaded or down and not accepting TCP connections...
dispatch.earliest_time = -15m@m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("The indexer not accepting TCP connections is either a serious performance issue or downtime")` \
index=_internal TcpOutputFd "connection refused" sourcetype=splunkd source=/opt/splunk/* \
| rex "Connect to (?P<clientip>[^:]+)" \
| top clientip \
| lookup dnslookup clientip \
| where count>10
disabled = 1

[IndexerLevel - Buckets rolling more frequently than expected]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 8 8 * * *
description = Chance the alert requires action? Moderate. Indexer level issues - Buckets are moving out of the warm state quicker than expected and this may (or may not) be an issue, this could indicate that hot is undersized or there are too many buckets in the warm area.
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","indextime"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = bar
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Buckets are moving from warm to cold very quickly and this could be an issue related to the sizing not been valid for the indexes...")` \
`indexerhosts` index=_internal "Will chill bucket" source="/opt/splunk/var/log/splunk/splunkd.log" sourcetype=splunkd "/db/db"  \
| rex "=/([^/]+/){6}(?P<indexname>[^/]+)"  \
| stats count by indexname \
| sort - count \
| where (count>20)
disabled = 1

[What Access Do I Have?]
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.track = 0
description = Determine the access of the currently logged in user
dispatch.earliest_time = @d
dispatch.latest_time = now
display.general.timeRangePicker.show = 0
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.show = 0
request.ui_dispatch_app = Global
request.ui_dispatch_view = search
search = | rest /services/authentication/users `searchheadsplunkservers`\
| search `comment("The splunk_server was added above as since admins have a different access level on indexers this query gets confused so now limited to search heads")`\
    [| rest /services/authentication/current-context/context `searchheadsplunkservers`\
    | head 1 \
    | fields username \
    | rename username AS title] \
| table title roles | rename title as user | mvexpand roles\
| join type=left roles \
    [rest /services/authorization/roles `searchheadsplunkservers`\
    | table title srchIndexesAllowed srchIndexesDefault | rename title as roles]\
| makemv srchIndexesAllowed tokenizer=(\S+) | makemv srchIndexesDefault tokenizer=(\S+)\
| fillnull value=" "\
| mvexpand srchIndexesAllowed | mvexpand srchIndexesDefault\
| join type=left max=999 srchIndexesAllowed \
    [ rest /services/data/indexes \
    | table title \
    | eval srchIndexesAllowed = if(match(title, "^_"), "_*", "*") \
    | rename title as IndexesAllowed]\
| join type=left max=999 srchIndexesDefault \
    [rest /services/data/indexes \
    | table title \
    | eval srchIndexesDefault = if(match(title, "^_"), "_*", "*") \
    | rename title as IndexesDefault]\
| stats values(*) as * by user\
| foreach srch* [eval <<FIELD>> = mvappend(<<FIELD>>, <<MATCHSTR>>) | eval <<FIELD>> = mvfilter(match(<<FIELD>>, "^[^*]+$"))]\
| fields - Indexes*

[ForwarderLevel - Read operation timed out expecting ACK]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 47 */2 * * *
description = Chance the alert requires action? Low. Acknowledgement from the indexers should ideally never timeout, the time out may cause duplication issues
dispatch.earliest_time = -2h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = index=_internal "Read operation timed out expecting ACK from" sourcetype=splunkd source=*/splunkd.log\
| rex "from (?P<indexer>\S+)"\
| stats count, max(_time) AS mostRecent by host, indexer  | eval mostRecent=strftime(mostRecent, "%+")
disabled = 1

[AllSplunkEnterpriseLevel - Replication Failures]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com 
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = */15 * * * *
description = Chance the alert requires action? Moderate. Replication failures often show a search head that is having issues after an indexer restart, the search head might require a restart to resolve this or investigation.
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","splunk_server"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Replication status failure on a search head, the search head may require a restart or investigation...")`\
index=_internal "because replication was unsuccessful. replicationStatus Failed failure info: failed_because" sourcetype=splunkd `splunkenterprisehosts` \
| stats count, max(_time) AS mostRecent by host, message\
| eval mostRecent=strftime(mostRecent, "%+") 
disabled = 1

[AllSplunkEnterpriseLevel - Low disk space]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com 
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = */5 * * * *
description = Chance the alert requires action? Moderate. Low disk space on one or more partitions of the Splunk enterprise servers...
dispatch.earliest_time = -5m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","splunk_server"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Use introspection data to monitor Splunk mount points, if you want to monitor non-Splunk directories use nmon or another monitoring system")`\
index=_introspection host=* component=Partitions\
| eval available='data.available', capacity='data.capacity', mount_point='data.mount_point'\
| eval percfree = round((available/capacity)*100,2)\
| stats min(percfree) AS percfree, min(available) AS minMBAvailable by mount_point, host\
| search `comment("Below 10% is an issue unless it's an indexer, as 10% of the indexer is actually a very large amount of data...")`\
(percfree<10 NOT (`indexerhosts`)) OR (minMBAvailable<90000 (`indexerhosts`))
disabled = 1

[AllSplunkEnterpriseLevel - KVStore Process Terminated]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com 
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 5
counttype = number of events
cron_schedule = */15 * * * *
description = Chance the alert requires action? High. Ideally you shouldn't see this error...
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","splunk_server"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("This ideally should never happen during normal runtime...")`\
index=_internal `searchheadhosts` "KV Store process terminated" sourcetype=splunkd source="/opt/splunk/var/log/splunk/splunkd.log"\
| fields _time, host, _raw
disabled = 1

[AllSplunkEnterpriseLevel - File integrity check failure]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 1
counttype = number of events
cron_schedule = 33 9 * * *
description = Chance the alert requires action? Moderate. File integrity check failure would generally mean a change has been made to parts of Splunk that will be wiped out next upgrade
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","splunk_server"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("One or more files did not pass the startup hash-check against the Splunk provided manifest, you can tune the limits.conf to control how the warning is logged or not logged")`\
index=_internal `splunkenterprisehosts` "An installed * did not pass hash-checking due to" sourcetype=splunkd\
| stats count, latest(_time) AS lastSeen by message, host\
| eval lastSeen=strftime(lastSeen, "%+")
disabled = 1

[AllSplunkEnterpriseLevel - WARN iniFile Configuration Issues]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com 
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 0 4 * * 1
description = Chance the alert requires action? Low. Detect configuration errors in the files that the indexer cluster or enterprise servers are throwing warnings about
dispatch.earliest_time = -1w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","splunk_server"]
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Find potentially invalid configuration within the Splunk applications on search heads/indexers and warn about this...")` \
index=_internal WARN IniFile `splunkenterprisehosts` sourcetype=splunkd\
| cluster showcount=true\
| fields _time, host, cluster_count, _raw
disabled = 1

[SearchHeadLevel - Long filenames may be causing issues]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com 
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 56 5 * * *
description = Chance the alert requires action? Moderate. There are one or more dashboards or alerts with a filename long enough to cause errors in the archive processor, the exact implications are unknown but the alert/dashboard may need to be removed.
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Detect the issue where someone has created a file longer than 100 characters and the cluster is having issues with replication. The 100 character issue was confirmed in Splunk 6.5.2 during a support case")`\
index=_internal `searchheadhosts`  ("ArchiveFile - Failed to write archive header for" "Pathname too long") OR ("ERROR Archiver - Unable to add entry")  source="/opt/splunk/var/log/splunk/splunkd.log"  sourcetype=splunkd \
| cluster showcount=true\
| fields _time, _raw, cluster_count
disabled = 1

[IndexerLevel - Large multiline events using SHOULD_LINEMERGE setting]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com 
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 2
counttype = number of events
cron_schedule = 42 7 * * *
description = Chance the alert requires action? Moderate. This alert advises that a multi-line event is appearing in Splunk that is large enough that the default SHOULD_LINEMERGE = true setting may cause blocking in the indexer aggregation queue, it's much more efficient to configure the SHOULD_LINEMERGE = false and LINE_BREAKER = ... if possible.\
Please update the props.conf for this sourcetype to LINE_BREAKER if applicable.
dispatch.earliest_time = -24h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","splunk_server"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = | tstats max(linecount) AS maxLineCount, count where index=*, linecount>250 groupby sourcetype\
| search `comment("This search detects sourcetypes with greater than 250 lines which have SHOULD_LINEMERGE set to true, this might cause blocking in the indexer aggregation queue if there are a large number")`\
`comment("of events with hundreds of lines or very large events such as >5000 lines of data. This alert is designed to give hints about where SHOULD_LINEMERGE=false / LINE_BREAKER=... might be more appropriate")`\
`comment("Note that the REST API will return every instance of sourcetype, it's not quite as accurate a btool so this can generate false alarms if there are multiple props.conf definitions of a sourcetype")`\
| join [| rest splunk_server=`splunkindexerhostsvalue` /servicesNS/-/-/configs/conf-props\
| fields title SHOULD_LINEMERGE\
| search SHOULD_LINEMERGE = 1\
| dedup title | rename title AS sourcetype]\
| where maxLineCount > 260 AND count>30
disabled = 1

[IndexerLevel - ERROR from linebreaker]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com 
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = */10 * * * *
description = Chance the alert requires action? High. This alert advises there is an error with the LINE_BREAKER, this generally relates to a misconfiguration that requires a fix...
dispatch.earliest_time = -10m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","splunk_server"]
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("LineBreakingProcessor ERROR's are usually related to misconfiguration/errors in the LINE_BREAKER= setup in props.conf and are therefore an issue")`\
index=_internal "ERROR LineBreakingProcessor"  sourcetype=splunkd | cluster | fields host, _raw
disabled = 1

[SearchHeadLevel - KVStore Or Conf Replication Issues Are Occurring]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com 
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
action.keyindicator.invert = 0
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = 12,22,32,42,52,02 * * * *
description = Chance the alert requires action? High. If the KVStore is out of sync or the search head is out of sync it will likely require a manual resync/clean to get it working as expected\
If it relates to a conf replication issue it is likely a problematic search head requiring a restart or it may require a force sync...(the logs will advise on this)
dispatch.earliest_time = -10m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.visualizations.chartHeight = 628
display.visualizations.charting.chart = area
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("Detect search head issues related to extended search head downtime, in particular ConfReplication issues or KV store replication issues")`\
`comment("KVStore - http://docs.splunk.com/Documentation/Splunk/latest/Admin/ResyncKVstore , ConfReplication - http://docs.splunk.com/Documentation/Splunk/latest/DistSearch/HowconfrepoworksinSHC#Replication_synchronization_issues")`\
`comment("The search head cluster captain is disconnected can relate to a SH cluster restart *or* if outside a rolling restart this may require a restart of the problematic search head...")`\
index=_internal `searchheadhosts` "Local KV Store has replication issues" OR "ConfReplicationThread - Error pulling configurations from captain" OR "ConfReplicationThread - The search head cluster captain * is disconnected; skipping configuration replication" sourcetype=splunkd source="/opt/splunk/var/log/splunk/splunkd.log" \
| cluster showcount=true \
| fields _time, host, _raw, message, cluster_count
disabled = 1

[SearchHeadLevel - SHCluster Artifact Replication Issues]
action.email = 1
action.email.include.search = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = updateme@company.com 
action.email.useNSSubject = 1
action.email.subject.alert = Sev $alert.severity$ Splunk Alert: $name$
alert.suppress = 0
alert.track = 1
alert.digest_mode = 1
alert.severity = 4
counttype = number of events
cron_schedule = 54 * * * *
description = In this scenario either something has changed or one or more search heads are not syncing the artifacts as expected, a restart of the SH cluster usually resolves this.
dispatch.earliest_time = -1h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype"]
display.general.type = statistics
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = SplunkAdmins
request.ui_dispatch_view = search
search = `comment("When this issue occurs it is likely related to some kind of issue post-restart of the indexer/search head cluster. Restarting the search head cluster appears to resolve the issue in 6.5.2")`\
index=_internal "ERROR SHCArtifactId - *This GUID does not match the member's current GUID" sourcetype=splunkd source=/opt/splunk/var/log/splunk/splunkd.log `searchheadhosts` \
| eventstats max(_time) AS lasterror, min(_time) AS firsterror\
| cluster showcount=true \
| table host, cluster_count, _raw, lasterror, firsterror\
| eval lasterror = strftime(lasterror, "%+"), firsterror = strftime(firsterror, "%+")
disabled = 1
